{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp         0\n",
      "Svr_detect        0\n",
      "Svr_connect       0\n",
      "Ss_request        0\n",
      "Ss_Established    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_diff_'+str(i)] = data[key].diff(i).fillna(method='bfill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_diff_back_'+str(i)] = data[key].diff(-i).fillna(method='ffill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_cum'] = data[key].cumsum().fillna(method='ffill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_meddiff'] = data[key].median() - data[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_mindiff'] = data[key].min() - data[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_maxdiff'] = data[key].max() - data[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_meandiff'] = data[key].mean() - data[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_48018/3081097130.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_stddiff'] = data[key].std() - processed_df[key+'_meandiff']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib \n",
    "from copy import copy\n",
    "\n",
    "def data2timeseries(data, n_timesteps):\n",
    "    processed_data = copy(data)\n",
    "    # translate into timeseries\n",
    "    timeseries = []\n",
    "    for i in range(processed_data.shape[0]):\n",
    "        # pad 0 for the initial steps\n",
    "        if i < n_timesteps-1:\n",
    "            pad = np.zeros((1, n_timesteps, processed_data.shape[1]))\n",
    "            for j in range(i+1):\n",
    "                pad[0, n_timesteps-1-j, :] = processed_data[i-j, ...]\n",
    "            timeseries.append(pad)\n",
    "        else:\n",
    "            timeseries.append(processed_data[np.newaxis, i-n_timesteps+1:i+1, ...])\n",
    "    processed_data = np.concatenate(timeseries, axis=0)\n",
    "    return processed_data\n",
    "\n",
    "def preprocess_dataframe(scaler_path='scaler_ip.pkl', scaler=RobustScaler()):\n",
    "    # 학습 데이터 읽기. 경로 설정에 주의 하세요!\n",
    "    data = pd.read_csv('data/IP/DHCP.csv')\n",
    "    \n",
    "    for key in sorted(list(data.keys())):\n",
    "        data[key] = data[key].fillna(method='ffill')\n",
    "        data[key] = data[key].fillna(0)\n",
    "    print(np.sum(data.isna()))\n",
    "    # -----------------------------------\n",
    "    # TODO: 데이터 분석을 통해 다양한 전처리를 시도 해보세요!\n",
    "    # preprocessed_train_set = train_set\n",
    "    # -----------------------------------\n",
    "    data['server_abnorm'] = data['Svr_detect'] + data['Svr_connect'] + data['Ss_request']\n",
    "    data['server_tot'] = data['Svr_detect'] + data['Svr_connect'] \n",
    "    data['client_tot'] = data['Ss_request'] + data['Ss_Established']\n",
    "    data['Svr_detect+Ss_request'] = data['Ss_request'] + data['Svr_detect'] \n",
    "    data['Svr_connet+Ss_request'] = data['Ss_request'] + data['Svr_connect'] \n",
    "    data['tot'] = data['server_tot']+data['client_tot']\n",
    "\n",
    "    processed_df = data.copy()\n",
    "    # data frame의 key를 set으로 변환하고 다시 list로 만드는 과정에서 key의 순서가 정해지지 않습니다. \n",
    "    # 본 검증 자료에서는 sort를 함으로써 동일한 결과가 나오도록 하였지만, 대회 중에는 이를 인지하지 못해 따로 통제하지 못했습니다. \n",
    "    # 이로 인해 검증 자료의 결과가 대회 기간 중 제출한 것과 완전히 동일하진 않을 수는 있으나, 결과의 유의미한 차이는 없을 것이라 \n",
    "    # 판단하여 자료를 제출하니 참고 부탁드립니다.\n",
    "    for key in sorted(list(set(data.keys())-set(['Timestamp']))):\n",
    "        processed_df[key+'_cum'] = data[key].cumsum().fillna(method='ffill').fillna(0)\n",
    "        processed_df[key+'_meddiff'] = data[key].median() - data[key]\n",
    "        processed_df[key+'_mindiff'] = data[key].min() - data[key]\n",
    "        processed_df[key+'_maxdiff'] = data[key].max() - data[key]\n",
    "        processed_df[key+'_meandiff'] = data[key].mean() - data[key]\n",
    "        processed_df[key+'_stddiff'] = data[key].std() - processed_df[key+'_meandiff']\n",
    "        for i in range(1, 7):\n",
    "            processed_df[key+'_diff_'+str(i)] = data[key].diff(i).fillna(method='bfill').fillna(0)\n",
    "            processed_df[key+'_diff_back_'+str(i)] = data[key].diff(-i).fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    processed_data = scaler.fit_transform(processed_df.drop(['Timestamp'], axis=1))\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    idx_half = data.index[data['Timestamp'] == '20210630_2350-0000'].tolist()[0]\n",
    "    test_out = processed_data[idx_half+1:]   # 7.1 - 12.31 분리\n",
    "\n",
    "    return processed_data, test_out, idx_half\n",
    "\n",
    "    \n",
    "train_set, test_set, idx_half = preprocess_dataframe(scaler=RobustScaler())\n",
    "train_ts = data2timeseries(train_set, 12)\n",
    "test_ts = data2timeseries(test_set, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make True-Positive train-valid set using Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39318, 12, 190), (9830, 12, 190), (26496, 12, 190))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def isolate_outliers(train_data_ts:np.ndarray, train_df:np.ndarray, idx_half:int, n_time_step=6, n_estimators=200, seed=415, contamination=0.01):\n",
    "    assert train_data_ts.shape[0]==len(train_df)\n",
    "    model = IsolationForest(n_estimators=n_estimators, random_state=seed, contamination=contamination)\n",
    "\n",
    "    indices = list(np.arange(len(train_df))[model.fit_predict(train_df)==-1])\n",
    "    new_indices = copy(indices)\n",
    "    for idx in indices:\n",
    "        for i in range(1, n_time_step):\n",
    "            if idx+i >= len(train_df):  \n",
    "                break\n",
    "            new_indices.append(idx+i)\n",
    "    new_indices = list(set(range(len(train_df)))-set(new_indices))\n",
    "    train_data_ts = train_data_ts[new_indices]\n",
    "    train, valid = train_test_split(train_data_ts, test_size=0.2, random_state=415)\n",
    "\n",
    "    return train, valid\n",
    "\n",
    "train_set, valid_set = isolate_outliers(train_ts, train_set, idx_half, contamination=0.01, n_time_step=12)\n",
    "train_set.shape, valid_set.shape, test_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39318, 12, 190) (9830, 12, 190) (26496, 12, 190)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/IP/ip_test_ts_12_190f.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_set.shape, valid_set.shape, test_ts.shape)\n",
    "joblib.dump(train_set, 'data/IP/ip_train_ts_12_190f.pkl')\n",
    "joblib.dump(valid_set, 'data/IP/ip_valid_ts_12_190f.pkl')\n",
    "joblib.dump(test_ts, 'data/IP/ip_test_ts_12_190f.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef59e2e189f317b9f1ce6402806fcf9d990888a75b01452c09c458c68dbcc631"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
