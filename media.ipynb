{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "- 실행환경: macOS 12.3   \n",
    "          Anaconda 4.12.0   \n",
    "          Python 3.9.7   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_diff_'+str(i)] = df[key].diff(i).fillna(method='bfill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_diff_back_'+str(i)] = df[key].diff(-i).fillna(method='ffill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_diffdays_'+str(i)] = df[key].diff(144*i).fillna(method='bfill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_diff_backdays_'+str(i)] = df[key].diff(-144*i).fillna(method='ffill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_cum'] = df[key].cumsum().fillna(method='ffill').fillna(0)\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_meddiff'] = df[key].median() - df[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_mindiff'] = df[key].min() - df[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_maxdiff'] = df[key].max() - df[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_meandiff'] = df[key].mean() - df[key]\n",
      "/var/folders/5f/1pffrx8s50n84qpr5nj8k9tr0000gn/T/ipykernel_43523/437199251.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df[key+'_stddiff'] = df[key].std() - processed_df[key+'_meandiff']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/Media/test_set_851.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib \n",
    "\n",
    "\n",
    "def preprocess_data(scaler_path='scaler_media.pkl', scaler=RobustScaler()):\n",
    "    # 학습 데이터 읽기. 경로 설정에 주의 하세요!\n",
    "    data_path = 'data/Media'\n",
    "    file_list = os.listdir(data_path)\n",
    "\n",
    "    df = []\n",
    "    for i, file in enumerate(file_list):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        data = pd.read_csv(file_path).fillna(method='ffill').fillna(0)\n",
    "        if i != 0:\n",
    "            del data['Timestamp']\n",
    "            data[f'{i}_tot'] = data.sum(axis=1)\n",
    "        else:\n",
    "            data[f'{i}_tot'] = data.drop(['Timestamp'], axis=1).sum(axis=1)\n",
    "        df.append(data)\n",
    "    df = pd.concat(df, axis=1)\n",
    "\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # data frame의 key를 set으로 변환하고 다시 list로 만드는 과정에서 key의 순서가 정해지지 않습니다. \n",
    "    # 본 검증 자료에서는 sort를 함으로써 동일한 결과가 나오도록 하였지만, 대회 중에는 이를 인지하지 못해 따로 통제하지 못했습니다. \n",
    "    # 이로 인해 검증 자료의 결과가 대회 기간 중 제출한 것과 완전히 동일하진 않을 수는 있으나, 결과의 유의미한 차이는 없을 것이라 \n",
    "    # 판단하여 자료를 제출하니 참고 부탁드립니다.\n",
    "    for key in sorted(list(set(df.keys())-set(['Timestamp']))):\n",
    "        processed_df[key+'_cum'] = df[key].cumsum().fillna(method='ffill').fillna(0)\n",
    "        processed_df[key+'_meddiff'] = df[key].median() - df[key]\n",
    "        processed_df[key+'_mindiff'] = df[key].min() - df[key]\n",
    "        processed_df[key+'_maxdiff'] = df[key].max() - df[key]\n",
    "        processed_df[key+'_meandiff'] = df[key].mean() - df[key]\n",
    "        processed_df[key+'_stddiff'] = df[key].std() - processed_df[key+'_meandiff']\n",
    "        for i in range(1, 7):\n",
    "            processed_df[key+'_diff_'+str(i)] = df[key].diff(i).fillna(method='bfill').fillna(0)\n",
    "            processed_df[key+'_diff_back_'+str(i)] = df[key].diff(-i).fillna(method='ffill').fillna(0)\n",
    "        for i in range(1, 3):\n",
    "            processed_df[key+'_diffdays_'+str(i)] = df[key].diff(144*i).fillna(method='bfill').fillna(0)\n",
    "            processed_df[key+'_diff_backdays_'+str(i)] = df[key].diff(-144*i).fillna(method='ffill').fillna(0)\n",
    "    print(np.sum(processed_df.isna().to_numpy().any()))\n",
    "    processed_data = scaler.fit_transform(processed_df.drop(['Timestamp'], axis=1))\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    # TODO: 예시코드 실행을 위한 Train_set/Test_set 분할입니다. 반드시 이 형태로 학습/테스트할 필요는 없습니다.\n",
    "    end_of_year = df.index[df['Timestamp'] == '20171231_2355-0000'].tolist()[0]\n",
    "    test_set = processed_data[end_of_year+1:]  # 2018 1.1 - 12.31 분리\n",
    "    return processed_data, test_set\n",
    "\n",
    "tot_data_set, test_set = preprocess_data()\n",
    "joblib.dump(tot_data_set, 'data/Media/'+'tot_data_set_851.pkl')\n",
    "joblib.dump(test_set, 'data/Media/'+'test_set_851.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습 및 추론(약 30분 소요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomalies: 291\n",
      "예측 결과. \n",
      "        Prediction\n",
      "0                1\n",
      "1                0\n",
      "2                0\n",
      "3                0\n",
      "4                0\n",
      "...            ...\n",
      "105115           1\n",
      "105116           1\n",
      "105117           1\n",
      "105118           1\n",
      "105119           1\n",
      "\n",
      "[105120 rows x 1 columns]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/media_isolation_forest.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "contamination = 0.0015\n",
    "estimators = 500\n",
    "model = IsolationForest(n_estimators=estimators,random_state=415, contamination=contamination)\n",
    "model.fit(tot_data_set)\n",
    "ans = model.predict(test_set)\n",
    "\n",
    "ans[ans==1] = 0\n",
    "ans[ans==-1] = 1\n",
    "print(f'anomalies: {np.sum(ans==1)}')\n",
    "answer = pd.DataFrame(ans, columns=['Prediction'])\n",
    "print(f'예측 결과. \\n{answer}\\n')  \n",
    "answer.to_csv(f'Media_answer_isolation_tree{estimators}_{contamination}_feat851.csv', index=False)  # 제출용 정답지 저장\n",
    "joblib.dump(model, 'models/media_isolation_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "contamination = 0.0015\n",
    "estimators = 500\n",
    "model = joblib.load('models/media_isolation_forest.pkl')\n",
    "test_set = joblib.load('data/Media/'+'test_set_851.pkl')\n",
    "ans = model.predict(test_set)\n",
    "\n",
    "ans[ans==1] = 0\n",
    "ans[ans==-1] = 1\n",
    "print(f'anomalies: {np.sum(ans==1)}')\n",
    "answer = pd.DataFrame(ans, columns=['Prediction'])\n",
    "print(f'예측 결과. \\n{answer}\\n')  \n",
    "answer.to_csv(f'Media_answer_isolation_tree{estimators}_{contamination}_feat851.csv', index=False)  # 제출용 정답지 저장"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef59e2e189f317b9f1ce6402806fcf9d990888a75b01452c09c458c68dbcc631"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
